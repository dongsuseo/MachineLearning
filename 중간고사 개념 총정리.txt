* 지도학습
정답을 주고 학습하는 것
학습 데이터로부터 생성한 모델을 이용하여 주어진 입력으로부터 출력을 예측
대표적인 기계학습 방법으로 분류 및 회귀 존재
분류는 미리 정의된 여러 클래스 레이블 중 하나를 예측하는 것
회귀는 연속적인 숫자를 예측하는 것

* KNN(K-최근접 이웃)
기계학습에서 분류 문제를 해결하기 위해 주로 사용되는 알고리즘
분류에서 참조하게 될 가장 가까운 이웃 K개의 점을 결정하여 다수결 원칙으로 진행
데이터들이 분류 작업에 잘 반영될 수 있도록 전처리 과정 필수
사례 중심 학습
   학습 데이터로 판별 함수를 학습하는 대신 데이터를 메모리에 저장
   데이터 분포를 표현하기 위한 파라미터를 추정하지 않음
   모델을 별도로 구축하지 않음(게으른 학습, Lazy Model)

* 거리 기반 평가
유클리드 거리 : 한 점과 다른 한 점 사이의 직선 거리
맨하탄 거리 : 맨하탄 거리의 블록을 걸을 떄 모습에 유래

* 분류 과정
다수결 방식 : 이웃 데이터들 중 빈도 기준 제일 많은 클래스를 새 데이터의 클래스로 분류
가중합 방식 : 가까운 이웃 정보에 좀 더 가중치를 부여하여 분류

* KNN 장점
학습 데이터 수가 많은 경우 매우 효율적이고, 학습 데이터 내 노이즈 영향을 크게 받지 않음
매우 간단한 방법이지만 높은 성능을 보여준다.
* KNN 단점
데이터 특성에 맞게 최적의 이웃 수 K와 거리 기반 평가 방식의 결정이 필요하다.
새로운 관측치와 학습 데이터 각각의 거리를 측정해야하므로 복잡도가 높다.

* 데이터 전처리 기법
데이터 실수화, 데이터 정제, 데이터 통합, 데이터 축소, 데이터 변환, 데이터 균형

* 데이터 변환
데이터가 가진 특성 간 스케일 차가 심하면 패턴을 찾는데 문제가 발생한다.
표준화 : 데이터가 표준정규분포의 속성을 가지도록 조정
정규화 : 데이터의 값을 [0,1]로 조정

* 데이터 불균형
분류 문제 해결 시, 특정 클래스의 관측치가 다른 클래스에 비해 매우 낮게 나타나는 경우
과소표집 : 다수 클래스의 표본을 임의로 데이터로부터 제거하는 것
과대표집 : 소수 클래스의 표본을 복제하여 이를 데이터에 추가하는 것

* 과대적합
모델 파라미터들을 학습 데이터에 너무 가깝게 맞췄을 경우 발생하는 문제
학습 데이터에 과하게 학습되어 실제 데이터에서는 오차가 증가하는 현상
* 과소적합
모델이 너무 단순하여, 학습 데이터의 특징 및 구조를 충분히 반영하지 못한 경우 발생
학습 데이터가 충분하지 않을 경우, 테스트 데이터가 학습 데이터 특징에 대한 유추가 힘듦

* 모델 최적화
모델이 너무 복잡하거나, 학습 데이터에 너무 최적화하여 과대적합일 경우 현실에서 사용하기 힘듦
모델이 너무 단순하거나, 학습이 충분하지 않아 과소적합일 경우에도 에러가 높아 사용할 수 없음

* 데이터셋 분할
학습 데이터 : 모델 학습하면서 파라미터를 찾기 위한 데이터
검증 데이터 : 학습이 완료된 모델을 검증하기 위한 데이터
테스트 데이터 : 학습 과정과 무관한 모델 성능을 평가하기 위한 데이터

* 계층적 샘플링(stratify = t)
샘플링 편향을 방지하기 위해 원데이터셋의 클래스 비율에 맞게 데이터셋을 분할

* 판다스 자료 구조
시리즈는 데이터 값의 1차원 벡터, 데이터 프레임은 2차원 구조를 의미한다.

* 회귀
연속적인 숫자를 예측하는 것
변수 간의 관련성을 분석하여, 측정된 데이터로부터 예측 수행
독립변수로부터 수학식을 통하여 종속변수의 값을 추정

* KNN 이웃회귀 (가중 회귀 계산은 5-1장에서 한번 보기)
가장 가까운 이웃 K개의 점을 결정하여, 그들의 값들로부터 예측값을 계산
단순회귀 : 가장 가까운 이웃들의 값들을 단순히 평균하여 추정하는 방식
가중회귀 : 각 이웃이 얼마나 가까운지에 따라 가중평균을 구해 거리가 가까울수록
데이터가 더 유사할 것이라고 보고 가중치를 부여하는 방식

* 결정계수 (5-1장 보기)
회귀 문제에서의 성능 측정 도구 (결정계수 = R^2)
R^2 = SSR / SST = 1 - SSE/SST    => SSE = SSR + SST

* 선형회귀
가장 직관적이고 간단한 직선으로 독립변수와 종속변수의 관계를 모델링하는 기법
주어진 데이터로부터 독립변수와 종속변수의 관계를 가장 잘 나타내는 직선 방정식을 찾는것

* 다항회귀 ( y = ax^2 + bx + c)
직선이 아닌 곡선으로 회귀 문제를 해결
차수가 높아질수록 모델 복잡도가 높아지기 때문에 학습 데이터에 과대적합의 위험성 존재

* 다중회귀 ( y = ax + by + c)
독립변수가 아닌 다중변수를 이용
의미없는 특징을 추가할 경우, 모델 복잡도만 높아짐
기존의 특징들의 조합으로 새로운 특징 생성 가능
특성의 개수를 크게 늘리면, 학습 데이터에 과대적합의 위험성 존재

* 로지스틱회귀
KNN분류와 비슷하게 확률 기반으로 데이터 분류
선형회귀와 비슷하게 선형 방정식을 사용하여 데이터를 분류
여러 특징을 사용할 경우 다중회귀와 비슷
데이터가 어떤 클래스에 속할 확률을 0과 1사이의 수로 계산하여, 높은 확률의 클래스로 분류

* 로지스틱함수
확률을 0과 1사이의 수로 변환하기 위하여 시그모이드 함수 사용
선형 방정식의 결과값이 큰 음수일 경우 0으로 근접하고, 큰 양수일 경우 1로 근접

* 엔트로피
불확실성, 무질서도, 새로운 정보량, 놀라움 등의 정도를 나타내는 값
발생확률이 낮은 사건일수록 정보량이 높음
엔트로피가 클수록, 데이터가 불확실하고 무질서하여 데이터가 잘 분리되어있지 않은 상태
엔트로피가 작을 수록 데이터가 잘 분리되어 있는 상태
엔트로피는 정보량의 기댓값, 평균 정보량으로 계산

* 교차-엔트로피 손실함수
로지스틱회귀 모델에서 사용하는 손실함수를 위해 크로스엔트로피 공식 사용
엔트로피에서의 실제정답 확률값만 사용하는 대신에, 크로스 엔트로피에서는 예측확률값만 사용
정보량 = 실제값과 예측값이 얼마나 근사한지의 정도

*수렴
모델이 충분이 학습되지 않으면 성능이 수렴되지 않음
과대적합과 과소적합을 회피하기 위하여 적절한 학습과정이 필요하다.

* 소프트맥스 함수
시그모이드 함수는 이진분류를 위해 한개의 선형 방정식 출력값을 0과 1사이의 값으로 표현
소프트맥스 함수는 다중분류를 위해 복수 개의 선형 방정식 출력값들을 0과 1사이의 값으로 표현
소프트맥스 출력값에 One-Hot Encoding 방식을 사용하면 최대값은 1, 나머지는 0으로 변환

* 교차-엔트로피 손실함수
정답값이 1인 클래스를 제외한 모든 항은 0

* 데이터 변환 => 위에서 다뤘음

* 판별 분석
분류와 차원축소 2개의 목적을 위해 사용되는 알고리즘
클래스별 공분산 구조가 비슷하다면 선형판별분석(LDA), 그렇지 않다면 이차판별분석(QDA) 사용
클래스 내 분산과 클래스 간 분산의 비율을 최대화하는 방식으로 차원 축소
직선 위 투영된 데이터들이 같은 클래스는 가깝게, 다른 클래스는 멀게 위치하도록 학습

* 결정경계
투영되는 선과 직교하고, 클래스 간 투영된 데이터가 겹치는 영역이 작은 지점
SB : 클래스 간 분산이 큰 지점      SW : 클래스 내 분산이 작은 지점

* 선형판별분석(LDA)
클래스를 구분하는데 기여할 수 있는 중요도가 높은 판별변수 결정
클래스 구분의 기준이 되는 독립변수들의 선형결합으로 구성된 판별함수 도출
도출된 판별함수에 의해 학습 데이터 분류의 정확도 분석
판별함수를 이용하여 새로운 데이터, 즉 테스트 데이터 예측

* 의사결정나무
학습 데이터 분석을 통해 데이터 속성으로부터 패턴을 찾아내서 분류문제를해결하는 모델
질문을 계속 던져 답을 결정하게 하는 방식
좋은 질문일수록 분류가 쉬워지고, 나쁜 질문일 수록 정확한 분류가 어려워짐
의사결정방식 과정의 표현법이 나무와 비슷하여 의사결정나무라고 불림

* 장점
조건문 형식으로 표현되기 때문에 모델을 쉽게 이해할 수 있고, 중요한 변수 선택에 유용
CART : 분류와 회귀 문제를 모두 해결한다

* 단점
정확한 모델을 만들기 위하여 비교적 많은 데이터와 시간이 필요하다
데이터의 변화에 민감하기 때문에, 학습 데이터와 테스트 데이터의 도메인이 유사해야함

* 의사결정나무 과정
여러 독립변수 중 한 개의 변수 선택 후 분류를 위한 기준 결정
기준에 따라서 복수 개의 자식 노드 구성
위 두 단계를 반복하면서 노드를 증가시키며, 나무 형태의 모델 형성

* 불순도
엔트로피 상태
순도가 높다는 것은 엔트로피가 낮은 상태
불순도가 높다는 것은 불확실성이 높다는 것이며, 엔트로피가 높은 상태임을 의미한다.
불순소가 낮아지는 방향으로, 엔트로피가 낮아지는 상태로 의사결정나무 모델 생성

* 정보이득
부모노드와 자식노드 간의 불순도 차이
정보이득이 최대가 되도록 데이터 분류 기준 결정
분할된 데이터들의 불순도가 작을수록 정보이득 증가

* 가지치기
의사결정나무 모델의 깊이가 깊을수록 학습 데이터에 과대적합될 가능성이 높음
가장 간단한 방법은 나무의 최대 깊이 지정





